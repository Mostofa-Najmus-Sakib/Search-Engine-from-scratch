{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "search_engine_finalwork.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mostofa-Najmus-Sakib/Search-Engine-from-scratch/blob/main/Codes/search_engine_finalwork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ebwn2OSVno8_"
      },
      "source": [
        "import warnings\n",
        "warnings.simplefilter('ignore')\n",
        "from google.colab import drive \n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rTXOcZFeqyv"
      },
      "source": [
        "cd /content/drive/Shared\\ drives/2021_IR_437_537/Submission/Project_1/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Hw-5rjE1Ngj"
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "import collections\n",
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "from collections import Counter\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "string.punctuation\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "pd.set_option('display.max_rows', 100)\n",
        "import pickle\n",
        "import heapq\n",
        "from collections import Counter\n",
        "import collections\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from functools import reduce"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Y1ysv5S1PPR"
      },
      "source": [
        "# df_original = pd.read_csv( 'project_1_Wiki_sample.csv')\n",
        "Q= pd.read_csv('querymergedata.csv')\n",
        "Q = Q[[ 'qid', 'AnonID', 'Query', 'QueryTime']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgmB1GXz1_1n"
      },
      "source": [
        "df = pd.read_pickle('preprocessed_2_4_6_merge_pickle.pkl')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "id": "dE2w_j0EgVep",
        "outputId": "90c7de84-b339-42cc-987b-bcc4a8e9433e"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "      <th>title</th>\n",
              "      <th>id</th>\n",
              "      <th>content_new</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Morocco–Saudi Arabia relations\\r\\n\\r\\nMoroccan...</td>\n",
              "      <td>Morocco–Saudi Arabia relations</td>\n",
              "      <td>1</td>\n",
              "      <td>[moroccan, saudi, arabian, relat, refer, curre...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Anthony United States Post Office\\r\\n\\r\\nThe A...</td>\n",
              "      <td>Anthony United States Post Office</td>\n",
              "      <td>2</td>\n",
              "      <td>[anthoni, unit, state, post, offic, list, nati...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Dumraon (Vidhan Sabha constituency)\\r\\n\\r\\nDum...</td>\n",
              "      <td>Dumraon  Vidhan Sabha constituency</td>\n",
              "      <td>3</td>\n",
              "      <td>[dumraon, vidhan, sabha, constitu, one, legisl...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Chaker Khazaal\\r\\n\\r\\nChaker Khazaal (born 28 ...</td>\n",
              "      <td>Chaker Khazaal</td>\n",
              "      <td>4</td>\n",
              "      <td>[chaker, khazaal, born, septemb, beirut, leban...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Vicente Pascual Pastor\\r\\n\\r\\nVicente Pascual ...</td>\n",
              "      <td>Vicente Pascual Pastor</td>\n",
              "      <td>5</td>\n",
              "      <td>[vicent, pascual, pastor, alcoy, june, alcoy, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>599995</th>\n",
              "      <td>W50\\r\\n\\r\\nW50 may refer to:</td>\n",
              "      <td>W50</td>\n",
              "      <td>599996</td>\n",
              "      <td>[w, may, refer]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>599996</th>\n",
              "      <td>Medical logic module\\r\\n\\r\\nA medical logic mo...</td>\n",
              "      <td>Medical logic module</td>\n",
              "      <td>599997</td>\n",
              "      <td>[medic, logic, modul, mlm, independ, unit, hea...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>599997</th>\n",
              "      <td>Paderborn Untouchables\\r\\n\\r\\nThe Untouchables...</td>\n",
              "      <td>Paderborn Untouchables</td>\n",
              "      <td>599998</td>\n",
              "      <td>[untouch, paderborn, full, name, untouch, pade...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>599998</th>\n",
              "      <td>Sussex Corner, New Brunswick\\r\\n\\r\\nSussex Cor...</td>\n",
              "      <td>Sussex Corner  New Brunswick</td>\n",
              "      <td>599999</td>\n",
              "      <td>[sussex, corner, popul, canadian, villag, king...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>599999</th>\n",
              "      <td>Fort Richardson (Arlington, Virginia)\\r\\n\\r\\nF...</td>\n",
              "      <td>Fort Richardson  Arlington  Virginia</td>\n",
              "      <td>600000</td>\n",
              "      <td>[fort, richardson, detach, redoubt, construct,...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>600000 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  content  ...                                        content_new\n",
              "0       Morocco–Saudi Arabia relations\\r\\n\\r\\nMoroccan...  ...  [moroccan, saudi, arabian, relat, refer, curre...\n",
              "1       Anthony United States Post Office\\r\\n\\r\\nThe A...  ...  [anthoni, unit, state, post, offic, list, nati...\n",
              "2       Dumraon (Vidhan Sabha constituency)\\r\\n\\r\\nDum...  ...  [dumraon, vidhan, sabha, constitu, one, legisl...\n",
              "3       Chaker Khazaal\\r\\n\\r\\nChaker Khazaal (born 28 ...  ...  [chaker, khazaal, born, septemb, beirut, leban...\n",
              "4       Vicente Pascual Pastor\\r\\n\\r\\nVicente Pascual ...  ...  [vicent, pascual, pastor, alcoy, june, alcoy, ...\n",
              "...                                                   ...  ...                                                ...\n",
              "599995                       W50\\r\\n\\r\\nW50 may refer to:  ...                                    [w, may, refer]\n",
              "599996  Medical logic module\\r\\n\\r\\nA medical logic mo...  ...  [medic, logic, modul, mlm, independ, unit, hea...\n",
              "599997  Paderborn Untouchables\\r\\n\\r\\nThe Untouchables...  ...  [untouch, paderborn, full, name, untouch, pade...\n",
              "599998  Sussex Corner, New Brunswick\\r\\n\\r\\nSussex Cor...  ...  [sussex, corner, popul, canadian, villag, king...\n",
              "599999  Fort Richardson (Arlington, Virginia)\\r\\n\\r\\nF...  ...  [fort, richardson, detach, redoubt, construct,...\n",
              "\n",
              "[600000 rows x 4 columns]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64Qz3dKxK_vj",
        "outputId": "dbddf91c-3c2c-456f-8f3b-7fd743a15af9"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(600000, 4)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWwastKnMGAt"
      },
      "source": [
        "df_merge_5 = pd.read_pickle('inverted_0_600000_merge.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RUl5GdsDx5Zi",
        "outputId": "10c2aecd-b4ce-4e77-b527-28c2723aea7f"
      },
      "source": [
        "len(df_merge_5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1134085"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gj_qTNCjwQzt"
      },
      "source": [
        "## Inverted index"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3T-49UvwRE-"
      },
      "source": [
        "# df_merge_1 = pd.read_csv('inverted_0_10000.csv')\n",
        "# df_merge_2 = pd.read_csv('inverted_10000_20000.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wQx7jupPwRR8"
      },
      "source": [
        "# def string_to_list_of_inverted_index(string_of_list):\n",
        "#   item_1 = string_of_list[1:-1].replace(\"'\", \"\").split(\", \")\n",
        "#   item_2 = [int(x) for x in item_1]\n",
        "#   return item_2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHxje-wvwRdN"
      },
      "source": [
        "# df_merge_5 = pd.concat([df_merge_1, df_merge_2]).reset_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXa_PiNqwRnP"
      },
      "source": [
        "# df_merge_5['index'] = df_merge_5['id'].apply(string_to_list_of_inverted_index)\n",
        "# df_merge_5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMWJuwZQwR7j"
      },
      "source": [
        "# df_merge_5 = df_merge_5.groupby([df_merge_5.word]).index.apply(sum).to_frame().reset_index()\n",
        "# df_merge_5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSu4Feq5wSI2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oNuXfmJ1UVf"
      },
      "source": [
        "## Text Preprocessing (Functions)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbmNTm2K1X27"
      },
      "source": [
        "#### First 2 function is added from the snippet generation portion\n",
        "\n",
        "\n",
        "def remove_title(title, content):\n",
        "  character_remove = len(title)\n",
        "  content = content[character_remove:]\n",
        "  return content\n",
        "\n",
        "def split_it(a):\n",
        "    return  re.sub('[\\r\\n\\r\\n]', ' ', a)\n",
        "\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    punctuationfree=\"\".join([i for i in text if i not in string.punctuation])\n",
        "    return punctuationfree\n",
        "\n",
        "def preprocess(sentence):# remove number\n",
        "    sentence=str(sentence)\n",
        "    rem_num = re.sub('[0-9]+', '', sentence)\n",
        "    return rem_num\n",
        "\n",
        "def preprocess1(sentence):\n",
        "    sentence= str(sentence)\n",
        "    rem_num = re.sub('[^a-zA-Z0-9]+',' ', sentence)\n",
        "    cleaned_string = re.sub('\\s+',' ', rem_num)\n",
        "    return cleaned_string \n",
        "\n",
        "def remove_stopwords(text):#stopword remove\n",
        "    output= [i for i in text if i not in stopwords]\n",
        "    return output\n",
        "\n",
        "#defining the object for Lemmatization\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "def lemmatizer(text):\n",
        "    lemm_text = [wordnet_lemmatizer.lemmatize(word) for word in text]\n",
        "    return lemm_text\n",
        "\n",
        "porter_stemmer = PorterStemmer()\n",
        "def stemming(text):\n",
        "    stem_text = [porter_stemmer.stem(word) for word in text]\n",
        "    return stem_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kK_7mSR1eak"
      },
      "source": [
        "## Query Suggestion (Functions)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yue9GFNT1nPH"
      },
      "source": [
        "# Q= pd.read_csv('querymergedata.csv')\n",
        "# Q = Q[[ 'qid', 'AnonID', 'Query', 'QueryTime']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vV7PFLIV1o3Z"
      },
      "source": [
        "def qs(ql):\n",
        "    \n",
        "    r, c = Q.shape\n",
        "    a = []\n",
        "\n",
        "    for i in range(r):\n",
        "        s1= Q.iloc[i,2]\n",
        "        jy= (s1.split())\n",
        "        if jy[0]==ql:                                               ############## for now we can focus in this\n",
        "            a.append(Q.iloc[i,0])\n",
        "    vq = pd.DataFrame(columns =['qid', 'AnonID', 'Query', 'QueryTime'])\n",
        "    \n",
        "    for i in a:\n",
        "        vq = vq.append(Q[Q['qid'] == i])\n",
        "    vq = vq[vq.groupby('AnonID').AnonID.transform('count') > 1] \n",
        "    ### removing all the sessions that appeared once (check with the prof)\n",
        "    r1,c1 = vq.shape\n",
        "    a1 = []\n",
        "    b1 = []#qid\n",
        "    \n",
        "    for i in range(r1):\n",
        "        s1= vq.iloc[i,2]\n",
        "        jy= (s1.split())\n",
        "    # print(len(jy))\n",
        "        l= len(jy)\n",
        "        if l==1:                                               ############## might need to change here, check again with the length\n",
        "        # print(vq.iloc[:,2])\n",
        "            a1.append(vq.iloc[i,0])\n",
        "            \n",
        "    vq_val = pd.DataFrame(columns =['qid', 'AnonID', 'Query', 'QueryTime'])\n",
        "    for i in a1:\n",
        "        vq_val = vq_val.append(vq[vq['qid'] == i])\n",
        "    p = vq_val['AnonID'].unique()\n",
        "    vq_val_2 = pd.DataFrame(columns =['qid', 'AnonID', 'Query', 'QueryTime'])\n",
        "    \n",
        "    for i in p:\n",
        "        vq_val_2 = vq_val_2.append(vq[vq['AnonID'] == i])\n",
        "        \n",
        "    unique_query = vq_val_2['Query'].value_counts()\n",
        "    \n",
        "    df_Score  = unique_query.to_frame()\n",
        "    \n",
        "    freq_first_query_word = unique_query.iloc[0]\n",
        "    \n",
        "    df_Score['Score'] =   df_Score['Query']  / freq_first_query_word\n",
        "    \n",
        "    u = df_Score.reset_index().head()\n",
        "    \n",
        "    \n",
        "    sug = u[['index','Score']]\n",
        "    \n",
        "    sug.rename(columns={'index':'Query suggestion'}, inplace=True)\n",
        "    \n",
        "    print(sug)\n",
        "    \n",
        "    wait = input('....Press any key to continue....')\n",
        "   # return(sug)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Lh5_1Ru1pDK"
      },
      "source": [
        "def query_suggestion(y):\n",
        "    \n",
        "    qu = nltk.word_tokenize(y)\n",
        "    \n",
        "    \n",
        "    print('Query suggestion')\n",
        "    \n",
        "        \n",
        "    l= qu[0].lower() \n",
        "    \n",
        "    qqq = qs(l)\n",
        "    \n",
        "    \n",
        "    \n",
        "    return qqq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_OpZ_421pOV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWh8Cyef1pX9"
      },
      "source": [
        "## Identifying candidate resource and relevance ranking (Functions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHmuh0i91pgr"
      },
      "source": [
        "#### Maxd"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OI8_BEzR1ppi"
      },
      "source": [
        "def maxis(names):\n",
        "    nameset=set(names)\n",
        "   # print(nameset)\n",
        "    d={name:names.count(name) for name in nameset}\n",
        "\n",
        "    d1 = {k: v for k, v in sorted(d.items(), key=lambda item: item[1])}\n",
        "    # print(d1)\n",
        "    if (len(d1))==0:\n",
        "      d1.clear()\n",
        "    #  print('empty')\n",
        "\n",
        "    else:\n",
        "\n",
        "      d2 =list(d1.values())[-1]\n",
        "      return d2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ClvZIih1qJQ"
      },
      "source": [
        "def query_preprocess(x):\n",
        "    df1 = pd.DataFrame({'real_query':x.splitlines()})\n",
        "    df1['real_query']= df1['real_query'].apply(lambda x:remove_punctuation(x))\n",
        "    df1['real_query'] = df1['real_query'].apply(lambda x: x.lower())#lower\n",
        "    df1['real_query'] = df1['real_query'].map(lambda s:preprocess(s))\n",
        "    df1['real_query'] = df1['real_query'].map(lambda s:preprocess1(s))\n",
        "    df1['real_query'] = df1.apply(lambda row: nltk.word_tokenize(row['real_query']), axis=1)#tokenize1\n",
        "    df1['real_query'] = df1['real_query'].apply(lambda x:remove_stopwords(x))\n",
        "    df1['real_query'] = df1['real_query'].apply(lambda x:lemmatizer(x))\n",
        "    df1['real_query'] = df1['real_query'].apply(lambda x: stemming(x))\n",
        "    for i in range(1):\n",
        "        qq = df1.iloc[0,0]\n",
        "    return qq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VCDiH58V0MB"
      },
      "source": [
        "def dr(x, qq, df, df_merge_3):\n",
        "    # merge query and inverted index\n",
        "    q_df= pd.DataFrame(qq, columns=['word'])\n",
        "    q_df_merge = q_df.merge(df_merge_3, on ='word')\n",
        "    d900 = pd.DataFrame(columns = ['content', 'title', 'id', 'content_new'])\n",
        "    total = len(df)\n",
        "    if (len( q_df_merge))==1:\n",
        "      m =   q_df_merge.iloc[0,1]\n",
        "      idf_bottom= (len(q_df_merge.iloc[0,1]))\n",
        "      idf = np.log2(total /  idf_bottom)\n",
        "    else:\n",
        "    \n",
        "      # print(q_df_merge)\n",
        "      m1 =[]# allid\n",
        "    #  d900 = pd.DataFrame(columns = ['content', 'title', 'id', 'content_new'])\n",
        "      \n",
        "      idf_bottom = []\n",
        "      for i in range (len(q_df_merge)):\n",
        "          m1.append(q_df_merge.iloc[i,1])\n",
        "          idf_bottom.append(len(q_df_merge.iloc[i,1]))\n",
        "\n",
        "      # for y in m1:\n",
        "      #   print('all result',y)\n",
        "     # print(m1)\n",
        "  #    print(len(m1))\n",
        "      \n",
        "    #  print('idf is',idf_bottom)\n",
        "     # total = len(df)\n",
        "    #  print('total',total)\n",
        "      idf = []\n",
        "      # print('idf bottom 2 values ',idf_bottom)\n",
        "      for no in idf_bottom:\n",
        "          g = np.log2(total / no)\n",
        "          idf.append(g)\n",
        "\n",
        "   #print('idf value:', idf)\n",
        "      m = list(reduce(set.intersection, [set(item) for item in m1 ]))\n",
        "  #find the doc id where the query terms appear\n",
        "   #   print('commom id is',m)\n",
        "   #   print(len(m))\n",
        "    # for i5 in m:\n",
        "    #         print('common collection matched is',i5)\n",
        "\n",
        "    print('idf value :',idf)\n",
        "    for i9 in m:\n",
        "        d900= d900.append(df[df['id']==i9])# get all doc where all query term appears\n",
        "    if (len(d900))==0:\n",
        "        print('No Resource Found')\n",
        "    else:\n",
        "        d231= d900.copy()\n",
        "        #print(d231)\n",
        "        d231.insert(0, 'new_id', range(1, 1+ len(d231)))\n",
        "        # print(len(d231))\n",
        "        d231.dropna(inplace= True)\n",
        "        # print(len(d231))\n",
        "        # try: \n",
        "        d231['maxd']= d231['content_new'].apply(maxis)\n",
        "        # print(len(d231))\n",
        "        d231.dropna(inplace= True)\n",
        "       \n",
        "\n",
        "        f = d231.reset_index(drop=True)\n",
        "        # print(f.iloc[108:111,3])\n",
        "        # print('f whole result', f)\n",
        "\n",
        "        \n",
        "        tf= []\n",
        "        for i in range(len(f)):\n",
        "       #   try:\n",
        "                names = f.iloc[i,4]\n",
        "                nameset=set(names)\n",
        "                d={name:names.count(name) for name in nameset}\n",
        "                #print(i)\n",
        "                d1 = {k: v for k, v in sorted(d.items(), key=lambda item: item[1])}\n",
        "                # print(d1)\n",
        "                c = 0\n",
        "                ii = 0\n",
        "                if (len(qq))!=1:\n",
        "\n",
        "                  for s in qq:\n",
        "                    if s in d1.keys():\n",
        "                  \n",
        "                  \n",
        "                        # print('len not 1 i is',s)\n",
        "                        value = d1[s]\n",
        "                        # print('value is :', value )\n",
        "                        # print('idf is :', idf )\n",
        "                      # print( s, 'frequency is',value)\n",
        "                        div = value*idf[ii]\n",
        "                       # print('div is',div)\n",
        "                        ii = ii + 1\n",
        "                        c= c+ div\n",
        "                       # print('sum is',c)\n",
        "                  tf.append(c)\n",
        "                else:\n",
        "                  for s in qq:\n",
        "                    if s in d1.keys():\n",
        "                  \n",
        "                  \n",
        "                        # print('inly one key',s)\n",
        "                        value = d1[s]\n",
        "                       # print('value is :', value )\n",
        "                        # print('idf is :', idf )\n",
        "                      # print( s, 'frequency is',value)\n",
        "                        div = value*idf\n",
        "                        # print(div)\n",
        "                        #ii = ii + 1\n",
        "                        # c= c+ div\n",
        "                      # print(c)\n",
        "                  tf.append(div)\n",
        "                #print('sum is', tf)\n",
        "\n",
        "         \n",
        "        o= len(tf)\n",
        "        tfcol = pd.DataFrame(np.array(tf).reshape(o,1), columns = list(\"a\"))\n",
        "        jk = pd.concat([f, tfcol], axis=1)\n",
        "      #  print(jk)\n",
        "        jk['score'] = jk['a']/jk['maxd']\n",
        "\n",
        "     #   print('raw dataframe',jk[['maxd','a','score']]) need to show professor\n",
        "\n",
        "###new\n",
        "        #idf = np.log(len(df)/len(jk))\n",
        "    #idf calculation\n",
        "        #jk['score'] =  jk['score'] *idf # multiply to calculate tfidf\n",
        "#######\n",
        "        jk = jk.sort_values([\"score\"], ascending=False)\n",
        "        jki = jk.reset_index(drop=True)\n",
        "\n",
        "        jkicol = jki[['maxd','a','score']]\n",
        "       # print\n",
        "\n",
        "        jki= jki[['id','content','title','score']]\n",
        "        totalrs = len(jki)\n",
        "        print(\"Total resource found : \" ,totalrs)\n",
        "        jk_is= jki.head()\n",
        "        jk_is.dropna(subset = [\"content\"], inplace=True)\n",
        "        # jk_is.to_csv('cosim.csv')\n",
        "        kkk = jk_is[['id','content']]\n",
        "        print('-----Top Relevant Documents-----')\n",
        "        print(kkk)\n",
        "        print('\\n')\n",
        "        hhhh = jk_is[['title','score']]\n",
        "        print(hhhh)\n",
        "        print('\\n')\n",
        "\n",
        "\n",
        "        query_frequency,que_tf, query1 = query_preprocess_for_cosine_similarity(x)\n",
        "#  print(query1)\n",
        "     \n",
        "        cosin= cosim(jk_is, query_frequency,que_tf, query1 )\n",
        "\n",
        "# print(cosin)\n",
        "        for cc in range(len(jk_is)):\n",
        "            print('\\n')\n",
        "            print('docid is')\n",
        "            print(cosin.iloc[cc,0])\n",
        "            print('\\n')\n",
        "            print('cosime similarity sentence')\n",
        "            print(cosin.iloc[cc,1])\n",
        "        wait = input('....Press any key to continue....')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ns3vaNEa1pyX"
      },
      "source": [
        "## Generating sinppets (Functions)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpgPmIfe26FF"
      },
      "source": [
        "def query_preprocess_for_cosine_similarity(query_as_string):\n",
        "  query_lower = remove_punctuation(query_as_string).lower()\n",
        "  query_remove_punc = preprocess1(query_lower)\n",
        "  query_tok = nltk.word_tokenize(query_remove_punc)\n",
        "  query_tok_stop_word_remove = remove_stopwords(query_tok)\n",
        "  query_tok_lemm = lemmatizer(query_tok_stop_word_remove)\n",
        "  query_tok_stemm = stemming(query_tok_lemm)\n",
        "  query1 = query_tok_stemm\n",
        "\n",
        "  query_frequency = collections.Counter(query1)\n",
        "  que_tf = {k: v / sum(query_frequency.values()) for k, v in query_frequency.items()}\n",
        "\n",
        "  return query_frequency, que_tf, query1\n",
        "\n",
        "\n",
        "\n",
        "def remove_title(title, content):\n",
        "  character_remove = len(title)\n",
        "  content = content[character_remove:]\n",
        "  return content\n",
        "\n",
        "def split_it(a):\n",
        "    return  re.sub('[\\r\\n\\r\\n]', ' ', a)\n",
        "\n",
        "def cal_quer_doc_freq(list_of_lists, query1):\n",
        "  list_containing_query_term = []\n",
        "  for list1 in list_of_lists:\n",
        "    list1_1 = remove_punctuation(list1).lower()\n",
        "    list1_2 = preprocess1(list1_1)\n",
        "    list_tok_1 =  nltk.word_tokenize(list1_2) \n",
        "    list_tok_stop_word_remove = remove_stopwords(list_tok_1)\n",
        "    list_tok_lemm = lemmatizer(list_tok_stop_word_remove)\n",
        "    list_tok_stemm = stemming(list_tok_lemm)\n",
        "    list_tok = list_tok_stemm\n",
        "\n",
        "    # list_tok =  nltk.word_tokenize(list1) ## only this line runs if any of the preprocess is not added \n",
        "\n",
        "    if any(item in list_tok for item in query1) == True:\n",
        "      query_in_sen = [x for x in list_tok if x in query1]\n",
        "      res = {idx : query_in_sen.count(idx) for idx in set(query1)}\n",
        "      res['Sentence'] = list1\n",
        "      res_copy = res.copy()\n",
        "      list_containing_query_term.append(res_copy)\n",
        "    \n",
        "  return list_containing_query_term\n",
        "\n",
        "\n",
        "def cal_term_freq_in_doc_across_sen(list_of_lists, query1):\n",
        "  list_containing_query_term_2 = []\n",
        "\n",
        "  for list1 in list_of_lists:\n",
        "    list1_1 = remove_punctuation(list1).lower()\n",
        "    list1_2 = preprocess1(list1_1)\n",
        "    list_tok_1 =  nltk.word_tokenize(list1_2) \n",
        "    list_tok_stop_word_remove = remove_stopwords(list_tok_1)\n",
        "    list_tok_lemm = lemmatizer(list_tok_stop_word_remove)\n",
        "    list_tok_stemm = stemming(list_tok_lemm)\n",
        "    list_tok = list_tok_stemm\n",
        "\n",
        "    # list_tok =  nltk.word_tokenize(list1) ## only this line runs if any of the preprocess is not added \n",
        "\n",
        "    if any(item in list_tok for item in query1) == True:   \n",
        "      query_in_sen = [x for x in list_tok if x in query1]\n",
        "      res = {idx : query_in_sen.count(idx) for idx in set(query1)}\n",
        "      res['Sentence'] = list1\n",
        "      res_copy = res.copy()\n",
        "      list_containing_query_term_2.append(res_copy)\n",
        "    \n",
        "\n",
        "    list_containing_query_term_on_all_sentence = []\n",
        "    newDict = {}\n",
        "\n",
        "\n",
        "    for item in list_containing_query_term_2:\n",
        "      item.pop(\"Sentence\", None)\n",
        "      for (key, value) in item.items():\n",
        "        if value >1:\n",
        "          newDict[key] = 1\n",
        "        else:\n",
        "          newDict[key] = value\n",
        "        newDict_copy = newDict.copy()\n",
        "      list_containing_query_term_on_all_sentence.append(newDict_copy)\n",
        "\n",
        "  counter = collections.Counter()\n",
        "  for d in list_containing_query_term_on_all_sentence: \n",
        "    counter.update(d)  \n",
        "  \n",
        "  return counter\n",
        "\n",
        "\n",
        "def que_doc_sum_across_doc(list_of_lists, query1):\n",
        "\n",
        "  list_containing_query_term_3 = []\n",
        "\n",
        "  for list1 in list_of_lists:\n",
        "\n",
        "    list1_1 = remove_punctuation(list1).lower()\n",
        "    list1_2 = preprocess1(list1_1)\n",
        "    list_tok_1 =  nltk.word_tokenize(list1_2) \n",
        "    list_tok_stop_word_remove = remove_stopwords(list_tok_1)\n",
        "    list_tok_lemm = lemmatizer(list_tok_stop_word_remove)\n",
        "    list_tok_stemm = stemming(list_tok_lemm)\n",
        "    list_tok = list_tok_stemm\n",
        "\n",
        "    # list_tok =  nltk.word_tokenize(list1) ## only this line runs if any of the preprocess is not added\n",
        "\n",
        "    if any(item in list_tok for item in query1) == True:   \n",
        "      query_in_sen = [x for x in list_tok if x in query1]\n",
        "      res = {idx : query_in_sen.count(idx) for idx in set(query1)}\n",
        "      res_copy = res.copy()\n",
        "      list_containing_query_term_3.append(res_copy)\n",
        "\n",
        "  counter = collections.Counter()\n",
        "  for d in list_containing_query_term_3: \n",
        "    counter.update(d)  \n",
        "  \n",
        "  return counter\n",
        "\n",
        "\n",
        "\n",
        "def doc_que_idf(counter_collec_dic,number_of_sentence_to_count):\n",
        "\n",
        "  number_of_sentence_in_collection = len(number_of_sentence_to_count)\n",
        "  d = dict(counter_collec_dic)\n",
        "  idf_value = {k: np.log( number_of_sentence_in_collection/(v+1) +1) for k, v in d.items()}\n",
        "  return idf_value\n",
        "\n",
        "\n",
        "def doc_tf(list_of_dic):\n",
        "\n",
        "  tf_list = []\n",
        "  for item in list_of_dic:\n",
        "\n",
        "    list1 = item['Sentence']\n",
        "    list1_1 = remove_punctuation(list1).lower()\n",
        "    list1_2 = preprocess1(list1_1)\n",
        "    list_tok_1 =  nltk.word_tokenize(list1_2) \n",
        "    list_tok_stop_word_remove = remove_stopwords(list_tok_1)\n",
        "    list_tok_lemm = lemmatizer(list_tok_stop_word_remove)\n",
        "    list_tok_stemm = stemming(list_tok_lemm)\n",
        "    list_tok = list_tok_stemm\n",
        "    length_of_sen = len(list_tok)\n",
        "\n",
        "\n",
        "    item.pop(\"Sentence\", None)\n",
        "    tf_value = {k: v / length_of_sen for k, v in item.items()}  \n",
        "    tf_list.append(tf_value)\n",
        "\n",
        "  return tf_list\n",
        "\n",
        "\n",
        "\n",
        "def doc_tf_idf(d_tf_list_of_dic, d_idf):\n",
        "\n",
        "  d_tf_idf = []\n",
        "\n",
        "  for item in d_tf_list_of_dic:\n",
        "    tf_idf = {k: v * d_idf[k] for k, v in item.items()} \n",
        "    d_tf_idf.append(tf_idf)\n",
        "  return d_tf_idf\n",
        "\n",
        "\n",
        "def query_tf_idf(q_tf_dic, que_idf):\n",
        "  q_tf_idf = {k: q_tf_dic[k]*que_idf[k] for k in que_idf}\n",
        "  return q_tf_idf\n",
        "\n",
        "\n",
        "\n",
        "def cosine_similarity(d_tf_idf, q_tf_idf):\n",
        "  final_cosine_similarity = []\n",
        "  for item in d_tf_idf:\n",
        "    cos_sim_numerator = [v * q_tf_idf[k] for k, v in item.items()]\n",
        "    cos_sim_numerator_without_nan = np.nansum(cos_sim_numerator)\n",
        "\n",
        "    cosine_sim_denomenator_d =  [v * v for k, v in item.items()]\n",
        "    cosine_sim_denomenator_q =  [q_tf_idf[k] * q_tf_idf[k] for k, v in item.items()]\n",
        "    cosine_sim_denomenator_dq = np.sqrt(np.nansum(cosine_sim_denomenator_d)) * np.sqrt(np.nansum(cosine_sim_denomenator_q))\n",
        "\n",
        "    cosine_similarity = cos_sim_numerator_without_nan/cosine_sim_denomenator_dq\n",
        "    final_cosine_similarity.append(cosine_similarity)\n",
        "    \n",
        "  return(final_cosine_similarity)\n",
        "\n",
        "\n",
        "def sentence_extraction(cos_similar_value, list_of_sen):\n",
        "  cos_similarity_max_value = heapq.nlargest(2, cos_similar_value)\n",
        "  index_list = []\n",
        "    \n",
        "  for item in cos_similarity_max_value:  \n",
        "    index = cos_similarity_max_value.index(item)\n",
        "    index_list.append(index)\n",
        "  sentence_list =[]\n",
        "  for ind in index_list:\n",
        "    sentence = list_of_sen[ind]['Sentence']\n",
        "\n",
        "    sentence_list.append(sentence)\n",
        "  #sentence_with_cosine_value = []\n",
        "\n",
        "  sentence_with_cosine_value = [cos_similarity_max_value, sentence_list]  ## removed cos_similarity_max_value as value was coming same, need to check the calculation\n",
        "  return(sentence_with_cosine_value)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7010FOUN26N7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlStWGFR7MSo"
      },
      "source": [
        "## Function to do all the preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fo20hBpI7Mm9"
      },
      "source": [
        "def text_preprocessing_all_function(df):\n",
        "\n",
        "  df[\"content_new\"] = df.apply(lambda x: remove_title(x[\"title\"], x[\"content\"]), axis = 1)\n",
        "  df['content_new'] = df['content_new'].apply(split_it)\n",
        "\n",
        "  df['content_new']= df['content_new'].apply(lambda x:remove_punctuation(x))\n",
        "  df['content_new'] = df['content_new'].apply(lambda x: x.lower())#lower\n",
        "  df['content_new'] = df['content_new'].map(lambda s:preprocess(s))\n",
        "  df['content_new'] = df['content_new'].map(lambda s:preprocess1(s))\n",
        "  df['content_new'] = df.apply(lambda row: nltk.word_tokenize(row['content_new']), axis=1)#tokenize\n",
        "  df['content_new'] = df['content_new'].apply(lambda x:remove_stopwords(x))\n",
        "  df['content_new'] = df['content_new'].apply(lambda x:lemmatizer(x))\n",
        "  df['content_new'] = df['content_new'].apply(lambda x: stemming(x))\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgKqxoj94LK4"
      },
      "source": [
        "## Function to run all the cosine similarity calculation functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChcuTidH26XB"
      },
      "source": [
        "def cosim(df_doc, query_frequency,que_tf, query1 ):\n",
        "   # query_frequency,que_tf, query1 = query_preprocess_for_cosine_similarity(original_query)\n",
        "    df_doc[\"content_w_o_title\"] = df_doc.apply(lambda x: remove_title(x[\"title\"], x[\"content\"]), axis = 1)\n",
        "    df_doc['content_clean'] = df_doc['content_w_o_title'].apply(split_it)\n",
        "    df_doc['con_clean_sen'] = df_doc['content_clean'].apply(sent_tokenize)\n",
        "#     df_doc[\"que_sen_freq\"] = df_doc[\"con_clean_sen\"].apply(cal_quer_doc_freq)\n",
        "    \n",
        "    df_doc[\"que_sen_freq\"] = df_doc.apply(lambda x: cal_quer_doc_freq(x['con_clean_sen'],query1), axis = 1)\n",
        "    \n",
        "#     df_doc[\"term_freq_in_sen\"] = df_doc[\"con_clean_sen\"].apply(cal_term_freq_in_doc_across_sen)\n",
        "    \n",
        "    df_doc[\"term_freq_in_sen\"] = df_doc.apply(lambda x: cal_term_freq_in_doc_across_sen(x[\"con_clean_sen\"],query1),axis=1)\n",
        "    \n",
        "    \n",
        "#     df_doc[\"term_freq_in_sen_doc\"] = df_doc[\"con_clean_sen\"].apply(que_doc_sum_across_doc)\n",
        "    \n",
        "    df_doc[\"term_freq_in_sen_doc\"] = df_doc.apply(lambda x: que_doc_sum_across_doc(x[\"con_clean_sen\"],query1),axis = 1)\n",
        "    \n",
        "    \n",
        "    \n",
        "    # df_doc[\"q_d_idf\"] = df_doc[\"term_freq_in_sen\"].apply(doc_que_idf)\n",
        "    df_doc[\"q_d_idf\"] = df_doc.apply(lambda x: doc_que_idf(x[\"term_freq_in_sen\"], x[\"que_sen_freq\"]), axis =1)\n",
        "    # df_doc[\"d_tf\"] = df_doc.apply(lambda x: doc_tf(x[\"que_sen_freq\"], x[\"term_freq_in_sen_doc\"]), axis = 1)\n",
        "    df_doc[\"d_tf\"] = df_doc[\"que_sen_freq\"].apply(doc_tf)\n",
        "    df_doc[\"d_tf_idf\"] = df_doc.apply(lambda x: doc_tf_idf(x[\"d_tf\"], x[\"q_d_idf\"]), axis = 1)\n",
        "    df_doc[\"q_tf_idf\"] = df_doc.apply(lambda x: query_tf_idf(que_tf, x[\"q_d_idf\"]), axis = 1)\n",
        "    df_doc[\"cos_similarity\"] = df_doc.apply(lambda x: cosine_similarity(x[\"d_tf_idf\"], x[\"q_tf_idf\"]), axis = 1)\n",
        "    \n",
        "#     df_doc[\"que_sen_freq_for_sen\"] = df_doc[\"con_clean_sen\"].apply(cal_quer_doc_freq)\n",
        "    \n",
        "    df_doc[\"que_sen_freq_for_sen\"] = df_doc.apply(lambda x: cal_quer_doc_freq(x[\"con_clean_sen\"], query1),axis = 1)\n",
        "    \n",
        "    \n",
        "    df_doc[\"cos_similarity_with_sentence\"] = df_doc.apply(lambda x: sentence_extraction(x[\"cos_similarity\"], x[\"que_sen_freq_for_sen\"]), axis = 1)\n",
        "    # df_doc= df_doc[['id','title', 'content', 'score', 'cos_similarity_with_sentence']] \n",
        "    df_doc= df_doc[['id', 'cos_similarity_with_sentence']] \n",
        "    #print('cos work',df_doc)\n",
        "    return df_doc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVEpL90226fQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJIO-nguq1Lq"
      },
      "source": [
        "# pd.set_option('display.height', 500)\n",
        "pd.set_option('display.max_rows', 7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyM9FKlE26uv"
      },
      "source": [
        "## Call functions for results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "8uf_l7HZnVr8",
        "outputId": "949f9ebe-adf4-4c97-ee90-599609459c1b"
      },
      "source": [
        "def main_menu():\n",
        "    while True:\n",
        "    \n",
        "      print('\\33[34m' \"-----     Search Engine     -----\")\n",
        "      \n",
        "      print('\\33[34m' \"\\n1.  Query suggestion\")\n",
        "      \n",
        "      print('\\33[34m'  \"\\n2. Relevant document\")\n",
        "    \n",
        "      choice = int(input('Enter your choice ...: '))\n",
        "      \n",
        "      if choice ==1:\n",
        "            \n",
        "            m= (input(\"Enter query for query suggestion:\"))\n",
        "            \n",
        "            query_suggestion(m)\n",
        "\n",
        "      if choice ==2:\n",
        "            \n",
        "            m1 =(input(\"Enter query to see relevance documents:\"))\n",
        "            l2 = query_preprocess(m1)\n",
        "       \n",
        "            dr(m1,l2,df, df_merge_5)\n",
        "                                                                     \n",
        "            \n",
        "      \n",
        "      if choice == 0:\n",
        "          print('end of work')\n",
        "          break\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main_menu()\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m-----     Search Engine     -----\n",
            "\u001b[34m\n",
            "1.  Query suggestion\n",
            "\u001b[34m\n",
            "2. Relevant document\n",
            "idf value : 5.547706805634007\n",
            "Total resource found :  12825\n",
            "-----Top Relevant Documents-----\n",
            "       id                                            content\n",
            "0  514576  Jochen Neerpasch\\r\\n\\r\\nJochen Neerpasch (born...\n",
            "1  297095  Courier (automobile)\\r\\n\\r\\nThe Courier was a ...\n",
            "2  291968  Ferrari TR\\r\\n\\r\\nThe Ferrari TR, or 250 Testa...\n",
            "3  492154  Welly (toy company)\\r\\n\\r\\nWelly Die Casting F...\n",
            "4  292818  Auto Avio Costruzioni 815\\r\\n\\r\\nThe Auto Avio...\n",
            "\n",
            "\n",
            "                       title     score\n",
            "0           Jochen Neerpasch  5.547707\n",
            "1       Courier  automobile   5.547707\n",
            "2                 Ferrari TR  5.547707\n",
            "3        Welly  toy company   5.547707\n",
            "4  Auto Avio Costruzioni 815  5.547707\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "docid is\n",
            "514576\n",
            "\n",
            "\n",
            "cosime similarity sentence\n",
            "[[1.0, 1.0], ['His racing career began in the 1960s, first on Borgward touring car, then with the 1964 24 Hours of Le Mans as a first major event.', 'His racing career began in the 1960s, first on Borgward touring car, then with the 1964 24 Hours of Le Mans as a first major event.']]\n",
            "\n",
            "\n",
            "docid is\n",
            "297095\n",
            "\n",
            "\n",
            "cosime similarity sentence\n",
            "[[1.0, 1.0], ['    The Courier was a brass era car manufactured by Sandusky Automobile Company in Sandusky, Ohio in 1904 and 1905.', '    The Courier was a brass era car manufactured by Sandusky Automobile Company in Sandusky, Ohio in 1904 and 1905.']]\n",
            "\n",
            "\n",
            "docid is\n",
            "291968\n",
            "\n",
            "\n",
            "cosime similarity sentence\n",
            "[[1.0, 1.0], ['    The Ferrari TR, or 250 Testa Rossa, is a race car model built by Ferrari in the 1950s and 1960s.', '    The Ferrari TR, or 250 Testa Rossa, is a race car model built by Ferrari in the 1950s and 1960s.']]\n",
            "\n",
            "\n",
            "docid is\n",
            "492154\n",
            "\n",
            "\n",
            "cosime similarity sentence\n",
            "[[1.0, 1.0], ['    Welly Die Casting Factory Limited is a manufacturer of die-cast toy cars and has been in the scale model business since 1979.', '    Welly Die Casting Factory Limited is a manufacturer of die-cast toy cars and has been in the scale model business since 1979.']]\n",
            "\n",
            "\n",
            "docid is\n",
            "292818\n",
            "\n",
            "\n",
            "cosime similarity sentence\n",
            "[[1.0, 1.0], ['    The Auto Avio Costruzioni 815 was the first car to be fully designed and built by Enzo Ferrari.', '    The Auto Avio Costruzioni 815 was the first car to be fully designed and built by Enzo Ferrari.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ep2lULlluFy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}